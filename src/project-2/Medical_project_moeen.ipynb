{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IN-STK5000/9000 - Medical Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Historical Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discovering structure in the data. It is uncertain if the symptoms present are all due to\n",
    "the same disease, or if they are different conditions with similar symptoms. (a) looking at the\n",
    "data (including symptoms), estimate whether a single-cause model is more likely than a multiplecause model. You can use anything, ranging from histograms or simple clustering algorithms\n",
    "to a hierarchical Bayesian model.   \n",
    "\n",
    "***  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From data readme  \n",
    "\n",
    "This is historical data in three tables.\n",
    "\n",
    "X: observations about each patient\n",
    "A: treatment\n",
    "Y: outcome of treatment\n",
    "\n",
    "The data is organised in the following files:\n",
    "\n",
    "historical.dat: all the tables in one file (matlab format)\n",
    "historical_X.dat: the X data\n",
    "historical_A.dat: the A data\n",
    "historical_Y.dat: the Y data\n",
    "\n",
    "Modelling the X data can be done through both unsupervised and supervised models. As some of the genome features might be irrelevant, it is probably a good idea to try and filter them out somehow. In later parts of the project, you will be able to perform experiments to narrow done the important genes. For the latter approach, you can combine the last two columns into a classification label, which should give you a cross-validation score of between 60-70%.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assumptions  \n",
    "every person has some disease"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "say something about confounder variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at clusters, see how they separate features, symptoms  \n",
    "\n",
    "Biplot on symptoms, look at eigenvalues for components  \n",
    "\n",
    "https://stackoverflow.com/questions/39216897/plot-pca-loadings-and-loading-in-biplot-in-sklearn-like-rs-autoplot\n",
    "\n",
    "np.cumsum(pca_model.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "# from pca import pca as PCA\n",
    "\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score   \n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.cm as cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for reproducibility\n",
    "SEED = 1337\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_path = \"../../data/medical/historical_X.dat\"\n",
    "\n",
    "X = pd.read_csv(feature_path, delimiter=\" \", names=[\"sex\", \"smoker\"] \\\n",
    "                                           + [f\"gen_{i}\" for i in range(1, 127)] \\\n",
    "                                           + [\"symptom_1\",\"symptom_2\"])\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task is to investigate whether there is a single disease or multiple diseases, and our best cues lies in the symptoms and their causes. For instance, sex and genetic data is assigned at birth, so we know that if there is a prominent correlation between those attributes and the symptoms, we know, assuming that symptoms are not before genes and sex, that these attributes cause the symptoms. However, we cannot assume such on sex. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Built into pandas, we can use the corr method to find the correlation matrix for each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_corr = X.corr()\n",
    "X_corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the most correlated features with respect to the symptoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond = (X_corr[\"symptom_1\"] > 0.3)  | (X_corr[\"symptom_1\"] < -0.3) | (X_corr[\"symptom_2\"] > 0.3) | (X_corr[\"symptom_2\"] < -0.3)\n",
    "most_significant = X_corr.loc[cond][[\"symptom_1\", \"symptom_2\"]][:-2]\n",
    "sns.heatmap(most_significant, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first symptom seems to be caused by genetical factors more than anything, both positively and negatively. The second symptom did not give us any highly correlated features, which might indicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = list(X.columns)\n",
    "X.corr()[[\"symptom_1\", \"symptom_2\"]][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reductor = PCA(n_components=2)\n",
    "x1, x2 = xT = reductor.fit_transform(X).T\n",
    "plt.scatter(x1, x2, edgecolor=\"k\", alpha=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# credit to https://towardsdatascience.com/pca-clearly-explained-how-when-why-to-use-it-and-feature-importance-a-guide-in-python-7c274582c37e\n",
    "def biplot(score, coeff , y):\n",
    "    '''\n",
    "    Author: Serafeim Loukas, serafeim.loukas@epfl.ch\n",
    "    Inputs:\n",
    "       score: the projected data\n",
    "       coeff: the eigenvectors (PCs)\n",
    "       y: the class labels\n",
    "   '''\n",
    "    xs = score[:,0] # projection on PC1\n",
    "    ys = score[:,1] # projection on PC2\n",
    "    n = coeff.shape[0] # number of variables\n",
    "    plt.figure(figsize=(10,8), dpi=100)\n",
    "    classes = np.unique(y)\n",
    "    colors = ['g','r','y']\n",
    "    markers=['o','^','x']\n",
    "    for s,l in enumerate(classes):\n",
    "        plt.scatter(xs[y==l],ys[y==l], c = colors[s], marker=markers[s]) # color based on group\n",
    "    for i in range(n):\n",
    "        #plot as arrows the variable scores (each variable has a score for PC1 and one for PC2)\n",
    "        plt.arrow(0, 0, coeff[i,0], coeff[i,1], color = 'k', alpha = 0.9,linestyle = '-',linewidth = 1.5, overhang=0.2)\n",
    "        plt.text(coeff[i,0]* 1.15, coeff[i,1] * 1.15, \"Var\"+str(i+1), color = 'k', ha = 'center', va = 'center',fontsize=10)\n",
    "\n",
    "    plt.xlabel(\"PC{}\".format(1), size=14)\n",
    "    plt.ylabel(\"PC{}\".format(2), size=14)\n",
    "    limx= int(xs.max()) + 1\n",
    "    limy= int(ys.max()) + 1\n",
    "    plt.xlim([-limx,limx])\n",
    "    plt.ylim([-limy,limy])\n",
    "    plt.grid()\n",
    "    plt.tick_params(axis='both', which='both', labelsize=14)\n",
    "    \n",
    "biplot(xT.T, np.transpose(reductor.components_[0:2, :]), np.zeros(len(x1)))\n",
    "reductor.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def kmeans_cluster_data(X, k=2, plot=True, fig=None, ax=None):\n",
    "    \"\"\"Takes in a dataset and clusters it for the n dimensions of the dataset.\n",
    "    If plot is enabled, the dataset is then reduced using pca before being plotted in a 2 dimensional space\n",
    "    Dimensions are kept for the clusterer\n",
    "    \n",
    "    returns labels\"\"\"\n",
    "    clusterer = KMeans(n_clusters=k, random_state=SEED)\n",
    "    labels = clusterer.fit_predict(X)\n",
    "    \n",
    "    if plot:\n",
    "        if not (fig or ax):\n",
    "            fig = plt.figure(figsize=(14, 7))\n",
    "            ax = fig.add_subplot()\n",
    "            \n",
    "        elif fig:\n",
    "            ax = fig.add_subplot()\n",
    "            \n",
    "        colors = cm.nipy_spectral(labels.astype(float) / k)\n",
    "            \n",
    "        ax.set_title(f\"{k}-means clustering\")\n",
    "#         pca = PCA(n_components=2)\n",
    "        pca = PCA(n_components=2)\n",
    "        \n",
    "        xp = pca.fit_transform(X)\n",
    "        centroids = pca.transform(clusterer.cluster_centers_)\n",
    "#         for i, c in enumerate(centroids):\n",
    "#             ax.scatter(*xp[labels==i].T, label=f\"label {i}\", s=30, c=colors, alpha=0.5)\n",
    "            \n",
    "        ax.scatter(*xp.T, s=30, c=colors, alpha=0.5)\n",
    "            \n",
    "        ax.scatter(*centroids.T, marker='o', c=\"white\", alpha=1, s=200, edgecolor='k')\n",
    "        for i, c in enumerate(centroids):\n",
    "            ax.scatter(c[0], c[1], marker='$%d$' % i, alpha=1, s=50, edgecolor='k')\n",
    "            \n",
    "#         ax.legend()\n",
    "    return labels, clusterer\n",
    "\n",
    "k4labels, _ = kmeans_cluster_data(X.to_numpy(), k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(solver=\"lbfgs\", multi_class=\"auto\", max_iter=5000)\n",
    "lr.fit(X, k4labels)\n",
    "\"linear seperability for cluster:\", lr.score(X, k4labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_n_clusters = [2, 3, 4, 5, 6]\n",
    "\n",
    "for n_clusters in range_n_clusters:\n",
    "    # Create a subplot with 1 row and 2 columns\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(18, 7)\n",
    "\n",
    "    # The 1st subplot is the silhouette plot\n",
    "    # The silhouette coefficient can range from -1, 1 but in this example all\n",
    "    # lie within [-0.1, 1]\n",
    "    ax1.set_xlim([-0.1, 1])\n",
    "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "    # plots of individual clusters, to demarcate them clearly.\n",
    "    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n",
    "\n",
    "    # Initialize the clusterer with n_clusters value and a random generator\n",
    "    # seed of 10 for reproducibility.\n",
    "    # clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n",
    "    # cluster_labels = clusterer.fit_predict(X)\n",
    "    \n",
    "    cluster_labels, clusterer = kmeans_cluster_data(X, k=n_clusters, ax=ax2)\n",
    "\n",
    "    # The silhouette_score gives the average value for all the samples.\n",
    "    # This gives a perspective into the density and separation of the formed\n",
    "    # clusters\n",
    "    silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "    print(\"For n_clusters =\", n_clusters,\n",
    "          \"The average silhouette_score is :\", silhouette_avg)\n",
    "\n",
    "    # Compute the silhouette scores for each sample\n",
    "    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
    "\n",
    "    y_lower = 10\n",
    "    for i in range(n_clusters):\n",
    "        # Aggregate the silhouette scores for samples belonging to\n",
    "        # cluster i, and sort them\n",
    "        ith_cluster_silhouette_values = \\\n",
    "            sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                          0, ith_cluster_silhouette_values,\n",
    "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "    # 2nd Plot showing the actual clusters formed\n",
    "    \n",
    "\n",
    "    ax2.set_title(\"The visualization of the clustered data.\")\n",
    "    ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
    "    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
    "    plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n",
    "                  \"with n_clusters = %d\" % n_clusters),\n",
    "                 fontsize=14, fontweight='bold')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the clustering part:\n",
    "* https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html#sphx-glr-auto-examples-cluster-plot-kmeans-silhouette-analysis-py\n",
    "* https://scikit-learn.org/stable/modules/biclustering.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do biclustering and grid-viz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Try and determine whether some particular factors are\n",
    "important for disease epidemiology and may require further investigations.\n",
    "You need to be able to validate your findings either through a holdout-set methodology,\n",
    "appropriately used statistical tests, or Bayesian model comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sb\n",
    "sb.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "X = pd.read_csv('../../data/medical/historical_X.dat', header=None, sep=\" \")\n",
    "A = pd.read_csv('../../data/medical/historical_A.dat', header=None, sep=\" \")\n",
    "Y = pd.read_csv('../../data/medical/historical_Y.dat', header=None, sep=\" \")\n",
    "X = X.rename(index=str, columns={0: 'gender', 1: 'smoker'})\n",
    "X = X.rename(index=str, columns={i: 'gene'+str(i-1) for i in range(2,128)})\n",
    "X_org = X.copy()\n",
    "X.insert(0, 'symptoms', X[128] + X[129]*2)\n",
    "X = X.drop(labels=[128, 129], axis=1)\n",
    "A = A.rename(index=str, columns={0: 'action'})\n",
    "Y = Y.rename(index=str, columns={0: 'outcome'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are assuming symptoms as our target variable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using L1 regularization on Logistic regression for reducing dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs = X.drop(labels=['symptoms'], axis=1)\n",
    "ys = X['symptoms']\n",
    "C = [10, 1, .5, .2, .1]\n",
    "for c in C:\n",
    "    clf = LogisticRegression(penalty='l1', C=c, solver='liblinear', multi_class='ovr')\n",
    "    fit = clf.fit(Xs, ys)\n",
    "    print((fit.coef_[:,:]>np.zeros(fit.coef_.shape[1])).sum(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how the features for C=0.1 (41,16,3,5) performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(fit.coef_[:,:]>np.zeros(fit.coef_.shape[1])).sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most features appear significant in one class only, a few in two but none in more than 2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Random Forest for finding feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from scipy.stats import sem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=1000)\n",
    "clf.fit(Xs, ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deviations = sem([tree.feature_importances_ for tree in clf.estimators_], axis=0)\n",
    "sorted_feature_ixs = np.argsort(clf.feature_importances_)[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the feature importance for all features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(range(Xs.shape[1]), clf.feature_importances_[sorted_feature_ixs],\n",
    "       color=\"r\", yerr=deviations[sorted_feature_ixs], ecolor='g')\n",
    "plt.xlabel('Features', fontsize=10)\n",
    "plt.ylabel('Feature importance score', fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that only few features are more important. Let's zoom in with only first 10 of the important ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(range(10), (clf.feature_importances_[sorted_feature_ixs])[:10],\n",
    "       color=\"r\", yerr=(deviations[sorted_feature_ixs])[:10], ecolor='g')\n",
    "plt.xticks(range(10), sorted_feature_ixs[:10])\n",
    "plt.xlabel('Feature number', fontsize=10)\n",
    "plt.ylabel('Feature importance score', fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the above graph, we can select the first four (i.e., 5,3,113,11) features which are clearly more relevant than others. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_imp = X.iloc[:,sorted_feature_ixs[:4]].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do clustering with only these important features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score, pairwise_distances\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.spatial.distance import sokalmichener"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = pd.concat([X_imp, X_org.iloc[:,-2:]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = pairwise_distances(X_new, metric=sokalmichener)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cluster = np.arange(1,9)\n",
    "scores = np.zeros(len(n_cluster))\n",
    "clusters = np.zeros([len(X_new), len(n_cluster)], dtype=np.int)\n",
    "for i, n in enumerate(n_cluster):\n",
    "    clusterer = AgglomerativeClustering(n_clusters=n, affinity='precomputed', \n",
    "                                        linkage=\"average\")\n",
    "    clusters[:,i] = clusterer.fit_predict(distances)\n",
    "for i in range(1, len(n_cluster)):\n",
    "    scores[i] = silhouette_score(distances, clusters[:,i], metric='precomputed')\n",
    "plt.bar(n_cluster[1:], scores[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shap values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_value = explainer.expected_value[0]\n",
    "shap_values = explainer.shap_values(Xs)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, Xs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Shap values  \n",
    "\n",
    "* Meaning of epidemiology? Do we predict disease or symptoms? Using which variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measuring the effect of actions. We also observe the effects of two different therapeutic\n",
    "interventions, one of which is placebo, and the other is an experimental drug. Try and measure\n",
    "the effectiveness of the placebo versus the active treatment. Are there perhaps cases where the\n",
    "active treatment is never effective, or should it always be recommended?\n",
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Synthetic control "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Improved Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 - Adaptive experiment design"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
