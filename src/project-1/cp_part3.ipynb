{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from group4_banker import Group4Banker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let \n",
    "\n",
    "$$\n",
    "\\pi_w(a \\mid x) = a \\left ( \\frac{\\exp(w^\\top x)}{\\exp(w^\\top x) + 1}\\right )\n",
    "$$\n",
    "\n",
    "be a policy parametrized by $w \\in \\mathbb{R}^n$, where $\\pi_w$ indicates the probability of whether to grant or not grant a loan given some feature vector $x \\in \\mathbb{R}^n$. The action $a$ is determined according to the expected utility $\\mathbb{E}(U)$ and is obtained by\n",
    "$$\n",
    "a = \\begin{cases}\n",
    "1 & \\text{ if } \\mathbb{E}(U) > 0 \\\\ \n",
    "0 & \\text{ if } \\mathbb{E}(U) \\leq 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "We remark that $\\pi_w$ is differentiable by $\\nabla_w \\pi_w = x\\exp(w^\\top x) / (\\exp(w^\\top x)  + 1)^2$, omitting $a$, which makes gradient based optimization a suitable approach to estimate $w$ under some objective. To maximise revenue $U$ and at the same time account for fairness $F$, we seek the parameters $w$ of $\\pi_w$ that maximizes\n",
    "\n",
    "$$\n",
    "\\int_{\\Theta} V_\\theta(\\pi_w) d\\beta(\\theta); \\quad V_\\theta(\\pi_w) = (1 - \\lambda)\\mathbb{E}_{\\pi_w}(U) - \\lambda \\mathbb{E}_{\\pi_w}(U)(F).\n",
    "$$\n",
    "\n",
    "Here, $\\lambda$ is a regularisation parameter balancing fairness and utility, $\\Theta$ is the set of possible inputs $\\theta \\sim \\Theta$ to a Random Forest (RF) model $\\beta$ that predicts the probability $P_\\theta$ of a particular outcome $y$ indicating wether a loan is being repaid or not. Moreover, $\\mathbb{E}_{\\pi_w}(U) = \\mathbb{I}(y = a_{\\pi_w})$ indicates model accuracy using $a_{\\pi_w} = \\arg \\max_a \\pi_w(a \\mid x)$ as model predictions. We define the expected fairness $\\mathbb{E}(F)$ as\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{\\pi_w}(U)(F) = \\sum_{y, a, z} \\pi_w(a \\mid x) \\left ( P_\\theta (a \\mid y, z) - P_\\theta (a \\mid y) \\right )^2\n",
    "$$\n",
    "\n",
    "where $z \\subset x$ denote some sensitive variables among the features. To optimize our objective, we approximate \n",
    "\n",
    "$$\n",
    "\\int_{\\Theta} V_\\theta(\\pi_w) d\\beta(\\theta) \\approx \\sum_{i=1}^n V_{\\theta^{(i)}}(\\pi_w),\n",
    "$$\n",
    "\n",
    "using Stochastic Gradient Descent (SGD) with automatic differentiation to obtain $\\nabla_{w} V_\\theta(\\pi_w)$. In each iteration of SGD we collect bootstrap samples  $\\theta^{(i)}$ from the validation/test set $\\Theta$ and predict $P_{\\theta^{(i)}}$ using the RF model $\\beta(\\theta^{(i)})$. Note that to distinguish betwen $P_\\theta (a \\mid y, z)$ and $P_\\theta (a \\mid y, z)$ we manage two RF models: one trained using all variables $x$ for in a boostrap sample $\\theta$ and another trained using only $x \\setminus z$ variables. Thats is, we exclude the sensitive variables $z$ when training the latter RF model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "\n",
    "def decision_makers(df: pd.DataFrame, df_nonsensitive: pd.DataFrame) -> Tuple:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        df: Dataset including all variables.\n",
    "        df_nonsensitive: Dataet excluding the sensitive variables.\n",
    "    \"\"\"\n",
    "        \n",
    "    # TODO:\n",
    "    # * Set interest rate etc.\n",
    "    # * Add w parameter to DM action method (defaults to zero).\n",
    "    dm_nonsensitive = None\n",
    "    dm_sensitive = None\n",
    "    \n",
    "    return dm_nonsensitive, dm_sensitive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def action(E_U: float) -> int:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        E_U: Expected utility.\n",
    "        \n",
    "    Returns:\n",
    "        Indicator for which action to make. \n",
    "    \"\"\"\n",
    "    \n",
    "    return int(E_U > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd_step(action, x, w, P_sensitives, P_non_sensitives, optimizer, lmbda) -> np.ndarray:\n",
    "    \n",
    "    # Optimize objective wrt. parameter w.\n",
    "    with tf.GradientTape() as tape:\n",
    "\n",
    "        # Just in case. \n",
    "        tape.watch(w)\n",
    "        \n",
    "        pi_w = None\n",
    "        \n",
    "        U = None\n",
    "        F = None\n",
    "        \n",
    "        # NOTE: Maximize V <=> minimize -1 * V.\n",
    "        dV_dw = optimizer.minimize((lmbda - 1) * U + lmbda * F, [w])\n",
    "    \n",
    "    # Cast to numpy so can be mixed with Python objects.\n",
    "    return w.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oob_estimate(w_i, X, dm_sensitive, dm_nonsensitive, size_oob=None):\n",
    "    \n",
    "    if size_oob is None:\n",
    "        size_oob = int(X.shape[0] * 1 / 3)\n",
    "    \n",
    "    # Bootstrap sample from validation/test data.\n",
    "    idx = np.random.choice(np.arange(X.shape[0]), replace=True, size=size_oob)\n",
    "\n",
    "    X_sub = X[test_idx]\n",
    "    y_sub = y[idx]\n",
    "\n",
    "    actions, P_sensitives, P_non_sensitives = [], [], []\n",
    "    for x in X_sub:\n",
    "\n",
    "        P_sensitives.append(dm_sensitive.get_best_action(w=w_i))\n",
    "        P_non_sensitives.append(dm_nonsensitive.get_best_action(w=w_i))\n",
    "\n",
    "    # Think we can do with absolute numbers but in case we need probabilities.\n",
    "    return np.mean(P_sensitives), np.mean(P_non_sensitives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment():\n",
    "    \n",
    "    lmbda = 1e-3\n",
    "\n",
    "    _w = 0\n",
    "    w = tf.Variable([0], dtype=tf.float32)\n",
    "    \n",
    "    # Train RF models on training data. \n",
    "    dm_nonsensitive, dm_sensitive = decision_makers()\n",
    "\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=1e-4)\n",
    "    for _ in range(num_epochs):\n",
    "        \n",
    "        P_sensitives, P_non_sensitives = oob_estimate(\n",
    "            \n",
    "        )\n",
    "        \n",
    "        _w = sgd_step(\n",
    "            w=w, P_sensitives=P_sensitives, P_non_sensitives=P_non_sensitives\n",
    "        )\n",
    "        \n",
    "    return w.numpy()\n",
    "\n",
    "\n",
    "experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data():\n",
    "\n",
    "    features = ['checking account balance', 'duration', 'credit history',\n",
    "                'purpose', 'amount', 'savings', 'employment', 'installment',\n",
    "                'marital status', 'other debtors', 'residence time',\n",
    "                'property', 'age', 'other installments', 'housing', 'credits',\n",
    "                'job', 'persons', 'phone', 'foreign']\n",
    "\n",
    "    target = 'repaid'\n",
    "\n",
    "    df_train = pd.read_csv(\"../../data/credit/D_train.csv\", sep=' ', names=features+[target])\n",
    "    df_test = pd.read_csv(\"../../data/credit/D_test.csv\", sep=' ', names=features+[target])\n",
    "\n",
    "    numerical_features = ['duration', 'age', 'residence time', 'installment', 'amount', 'persons', 'credits']\n",
    "    quantitative_features = list(filter(lambda x: x not in numerical_features, features))\n",
    "    D_train = pd.get_dummies(df_train, columns=quantitative_features, drop_first=True)\n",
    "    D_test = pd.get_dummies(df_test, columns=quantitative_features, drop_first=True)\n",
    "    encoded_features = list(filter(lambda x: x != target, D_train.columns))\n",
    "\n",
    "    return D_train, D_test, encoded_features, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_train, D_test, encoded_features, target = prep_data()\n",
    "\n",
    "X_train = D_train.loc[:, encoded_features] \n",
    "y_train = D_train.loc[:, target] \n",
    "\n",
    "model = Group4Banker(optimize=False, random_state=42)\n",
    "model.set_interest_rate(0.05)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "X_test = D_test.loc[:, encoded_features] \n",
    "y_test = D_test.loc[:, target] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
