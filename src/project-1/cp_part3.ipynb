{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from group4_banker import Group4Banker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let \n",
    "\n",
    "$$\n",
    "\\pi_w(x) = \\frac{\\exp(w^\\top x)}{\\exp(w^\\top x) + 1}\n",
    "$$\n",
    "\n",
    "be a policy parametrized by $w \\in \\mathbb{R}^n$, where $\\pi_w$ indicates the probability of whether to grant or not grant a loan given some feature vector $x \\in \\mathbb{R}^n$. We remark that $\\pi_w$ is differentiable by $\\nabla_w \\pi_w = x\\exp(w^\\top x) / (\\exp(w^\\top x)  + 1)^2$, making gradient based optimization a suitable approach to estimate $w$. To maximise revenue $U$ and at the same time account for fairness $F$, we seek the parameters $w$ of $\\pi_w$ that solves\n",
    "\n",
    "$$\n",
    "\\max_{\\pi_w} \\int_{\\Theta} V_\\theta(\\pi_w) d\\beta(\\theta); \\quad V_\\theta(\\pi_w) = (1 - \\lambda)\\mathbb{E}_{\\pi_w}(U \\mid x)  - \\lambda \\mathbb{E}_{\\pi_w}(U \\mid x, z)(F).\n",
    "$$\n",
    "\n",
    "Here, $\\lambda$ is a regularisation parameter balancing fairness and utility, $\\Theta$ is the set of possible inputs $\\theta \\sim \\Theta$ to a Random Forest (RF) model $\\beta$ that predicts the probability $P_\\theta$ of a particular outcome $y$ indicating whether a loan is being repaid or not. To quantify model performance, we optimize with respect to $\\mathbb{E}_{\\pi_w}(U \\mid x) = 1 / \\exp(- (y - \\pi_w(x))^2)$. This measure is maximized for $y = a_{\\pi_w}$ in which case $\\mathbb{E}_{\\pi_w}(U \\mid x)  = 1$, and the worst outcome is for $y = 0$ and $a_{\\pi_w} = 1$ (or vice versa) for which $\\mathbb{E}_{\\pi_w}(U \\mid x)  = 1 / e$. Moreover, we define the expected fairness $\\mathbb{E}(F)$ as\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{\\pi_w}(U \\mid x, z) (F) = \\sum_{y, a, z} \\pi_w(x) \\left ( P_\\theta (y \\mid x, z) - P_\\theta (y \\mid x) \\right )^2\n",
    "$$\n",
    "\n",
    "where $z \\subset x$ denote some sensitive variables among the features. To optimize our objective, we approximate \n",
    "\n",
    "$$\n",
    "\\int_{\\Theta} V_\\theta(\\pi_w) d\\beta(\\theta) \\approx \\frac{1}{n} \\sum_{i=1}^n V_{\\theta^{(i)}}(\\pi_w),\n",
    "$$\n",
    "\n",
    "using Stochastic Gradient Descent (SGD) with automatic differentiation to obtain $\\nabla_{w} V_\\theta(\\pi_w)$. In each iteration of SGD we collect bootstrap samples  $\\theta^{(i)}$ from the validation/test set $\\Theta$ and predict $P_{\\theta^{(i)}}$ using the RF model $\\beta(\\theta^{(i)})$. Note that to distinguish betwen $P_\\theta (a \\mid y, z)$ and $P_\\theta (a \\mid y, z)$ we manage two RF models: one trained using all variables $x \\cup z$ for in a boostrap sample $\\theta$ and another trained using only $x$ variables. Thats is, we exclude the sensitive variables $z$ when training the latter RF model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DTYPE = tf.float23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd_step(x, y, w, delta_P, optimizer, lmbda=0.01) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        action:\n",
    "        x: Feature vector.\n",
    "        w: Feature parameters to be learned.\n",
    "        \n",
    "    Returns:\n",
    "        Updated estimate of parameter vector w.\n",
    "    \"\"\"\n",
    "    \n",
    "    # NOTE: TF is sensitive about dtypes.\n",
    "    x = tf.cast(x, dtype=DTYPE)\n",
    "    w = tf.cast(w, dtype=DTYPE)\n",
    "    lmbda = tf.cast(lmbda, dtype=DTYPE)\n",
    "    delta_P = tf.cast(delta_P, dtype=DTYPE)\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "\n",
    "        # Just in case. \n",
    "        tape.watch(w)\n",
    "        \n",
    "        # Parametrized policy.\n",
    "        pi_w = tf.exp(tf.matmul(w, x)) / (tf.exp(tf.matmul(w, x)) + 1)\n",
    "        \n",
    "        # NOTE: Maximize V <=> minimize -1 * V.\n",
    "        V = (lmbda - 1) * 1 / tf.exp(tf.square(y - pi_w)) + lmbda * tf.reduce_sum(pi_w * delta_P)\n",
    "    \n",
    "    optimizer.minimize(V, [w])\n",
    "    \n",
    "    # Cast to numpy so can be mixed with Python objects.\n",
    "    return w.numpy(), -1.0 * V.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(X, y, seed=42, lmbda=1e-3):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        X: Feature matrix of N samples x P features.\n",
    "        y: Ground truths.\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Initial parameter estimate and optimization object.\n",
    "    w_i = np.random.random(X.shape[1])\n",
    "    w_i_tf = tf.Variable(w_i, dtype=DTYPE)\n",
    "    \n",
    "    # Train each of the RF models. \n",
    "    rf_x.train()\n",
    "    rf_xz.train()\n",
    "    \n",
    "    # Deviation from the RF model being independent on sensitive variable z.\n",
    "    delta_p = rf_x.predict(X) - rf_xz.predict(X) ** 2\n",
    "\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=1e-4)\n",
    "    \n",
    "    V_est = 0\n",
    "    for _ in range(num_epochs):\n",
    "        \n",
    "        i = np.random.choice(np.arange(X.shape[0]))        \n",
    "        w_i, V = sgd_step(x=X[i], y=y[i], w=w_i_tf, delta_P=delta_p, optimizer=optimizer)\n",
    "        \n",
    "        V_est += V\n",
    "                       \n",
    "    return w_i, V_est\n",
    "        \n",
    "\n",
    "experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data():\n",
    "\n",
    "    features = ['checking account balance', 'duration', 'credit history',\n",
    "                'purpose', 'amount', 'savings', 'employment', 'installment',\n",
    "                'marital status', 'other debtors', 'residence time',\n",
    "                'property', 'age', 'other installments', 'housing', 'credits',\n",
    "                'job', 'persons', 'phone', 'foreign']\n",
    "\n",
    "    target = 'repaid'\n",
    "\n",
    "    df_train = pd.read_csv(\"../../data/credit/D_train.csv\", sep=' ', names=features+[target])\n",
    "    df_test = pd.read_csv(\"../../data/credit/D_test.csv\", sep=' ', names=features+[target])\n",
    "\n",
    "    numerical_features = ['duration', 'age', 'residence time', 'installment', 'amount', 'persons', 'credits']\n",
    "    quantitative_features = list(filter(lambda x: x not in numerical_features, features))\n",
    "    D_train = pd.get_dummies(df_train, columns=quantitative_features, drop_first=True)\n",
    "    D_test = pd.get_dummies(df_test, columns=quantitative_features, drop_first=True)\n",
    "    encoded_features = list(filter(lambda x: x != target, D_train.columns))\n",
    "\n",
    "    return D_train, D_test, encoded_features, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_train, D_test, encoded_features, target = prep_data()\n",
    "\n",
    "X_train = D_train.loc[:, encoded_features] \n",
    "y_train = D_train.loc[:, target] \n",
    "\n",
    "model = Group4Banker(optimize=False, random_state=42)\n",
    "model.set_interest_rate(0.05)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "X_test = D_test.loc[:, encoded_features] \n",
    "y_test = D_test.loc[:, target] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
